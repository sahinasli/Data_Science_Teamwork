{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning and Vectorization For NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- `sent_tokenize`\n",
    "- `word_tokenize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "- Import necessary packages\n",
    "- `Split` text into `sentences` using NLTK\n",
    "- `Split` text into `words` using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of data science, extracting meaningful insights from vast datasets is an art. From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques. Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences. This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore. The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.\n"
     ]
    }
   ],
   "source": [
    "text = \"In the realm of data science, extracting meaningful insights from vast datasets is an art. From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques. Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences. This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore. The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Split text into sentences using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Split text into words using NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "- Get the word tokens `without punctuation and numbers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Get the word tokens without punctuation and numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "- Import necessary packages\n",
    "- Take a look at the `stop words` included in `nltk's corpus! (English)`\n",
    "- `Remove` stop words\n",
    "- `Print the stop words` included in the sample `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Take a look at the stop words included in nltk's corpus!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Print the stop words included in the sample text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "- `WordNetLemmatizer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "- Import necessary packages\n",
    "- `Reduce words` to their `root form` with `WordNetLemmatizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Reduce words to their root form with WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "- `PorterStemmer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "- Import necessary packages\n",
    "- `Reduce words` to their `stems` with `PorterStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Reduce words to their stems with PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Together (Cleaning Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the realm of data science, extracting meaningful insights from vast datasets is an art. From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques. Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences. This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore. The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Apply below `cleaning` function to `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "\n",
    "    #1. Tokenize and lower\n",
    "    text_tokens = word_tokenize(data.lower())\n",
    "\n",
    "    #2. Remove Puncs and numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "\n",
    "    #3. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "\n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "\n",
    "    #5. joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Clean the text with CLEANING function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer (Bag of Words)\n",
    "\n",
    "- `CountVectorizer` - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the realm of data science, extracting meaningful insights from vast datasets is an art. From predictive modeling to natural language processing (NLP), the field encompasses a spectrum of techniques. Consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences. This process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore. The synergy of NLP and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the realm of data science, extracting meaningful insights from vast datasets is an art.', 'from predictive modeling to natural language processing (nlp), the field encompasses a spectrum of techniques.', 'consider a scenario where a machine learning algorithm analyzes customer behavior, tokenizing text data to discern patterns in user preferences.', 'this process involves breaking down textual information into tokens, unlocking a treasure trove of information for data scientists to explore.', 'the synergy of nlp and data science opens doors to innovation, driving advancements in fields like recommendation systems, sentiment analysis, and beyond.']\n"
     ]
    }
   ],
   "source": [
    "corpus = sent_tokenize(text.lower())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Create a function `tokenize` like cleaning function that takes text and applies the following:\n",
    "- convert to all lowercase,\n",
    "- punctuation removal,\n",
    "- numbers removal,\n",
    "- word tokenization, \n",
    "- lemmatization, \n",
    "- and stop word removal using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Apply `CountVectorizer` by doing followings:\n",
    "- Import necesary packages,\n",
    "- Initialize `count_vectorizer` object (Use `TOKENIZE` function as tokenizer),\n",
    "- Create `X` object by applying `fit_transform` with `count_vectorizer` object to `corpus`,\n",
    "- Convert `sparse matrix (X)` to `numpy array` to view,\n",
    "- View token vocabulary and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Initialize count vectorizer object (Use TOKENIZE function as tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Create X object by applying fit_transform with count_vectorizer object to corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Convert sparse matrix (X) to numpy array to view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# View token vocabulary and counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfTransformer\n",
    "\n",
    "- `TfidfTransformer` - TF-IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Apply `TfidfTransformer` by doing followings:\n",
    "- Import necesary packages,\n",
    "- Initialize `tfidf_transformer` object,\n",
    "- Apply `fit_transform` with `tfidf_transformer` object to X,\n",
    "- Convert `sparse matrix` to `numpy array` to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Initialize tfidf_transformer object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Apply fit_transform with tfidf_transformer object to X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Convert sparse matrix to numpy array to view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "- `TfidfVectorizer` - Bag of Words AND TF-IDF values\n",
    "- `TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**\n",
    "\n",
    "Apply `TfidfTransformer` by doing followings:\n",
    "- Import necesary packages,\n",
    "- Initialize `tfidf_vectorizer` object,\n",
    "- Create `X` object by applying `fit_transform` with `tfidf_vectorizer` object to `corpus`,\n",
    "- Convert `sparse matrix (X)` to `numpy array` to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Import necesary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Initialize `tfidf_vectorizer` object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Create `X` object by applying `fit_transform` with `tfidf_vectorizer` object to `corpus`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Convert sparse matrix (X) to numpy array to view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job! Congrats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
